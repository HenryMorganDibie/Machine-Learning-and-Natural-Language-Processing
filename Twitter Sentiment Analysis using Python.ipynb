{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6fe9a51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The tweet column in the below dataset contains the tweets that we need to use to analyze the feelings of those engaged in the discussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af5a88ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  count  hate_speech  offensive_language  neither  class  \\\n",
      "0           0      3            0                   0        3      2   \n",
      "1           1      3            0                   3        0      1   \n",
      "2           2      3            0                   3        0      1   \n",
      "3           3      3            0                   2        1      1   \n",
      "4           4      6            0                   6        0      1   \n",
      "\n",
      "                                               tweet  \n",
      "0  !!! RT @mayasolovely: As a woman you shouldn't...  \n",
      "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...  \n",
      "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...  \n",
      "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...  \n",
      "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import re\n",
    "import nltk\n",
    "import nltk\n",
    "\n",
    "data = pd.read_csv(\"https://raw.githubusercontent.com/amankharwal/Website-data/master/twitter.csv\")\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1fe856e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Clean up a lot of errors and other special symbols because these tweets contain a lot of language errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5d1ef114",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Henry\n",
      "[nltk_data]     Morgan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stemmer = nltk.SnowballStemmer(\"english\")\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "stopword=set(stopwords.words('english'))\n",
    "\n",
    "def clean(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    text = [word for word in text.split(' ') if word not in stopword]\n",
    "    text=\" \".join(text)\n",
    "    text = [stemmer.stem(word) for word in text.split(' ')]\n",
    "    text=\" \".join(text)\n",
    "    return text\n",
    "data[\"tweet\"] = data[\"tweet\"].apply(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa37a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "## calculate the sentiment scores of these tweets and assign a label to the tweets as positive, negative, or neutral."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6556da17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to C:\\Users\\Henry\n",
      "[nltk_data]     Morgan\\AppData\\Roaming\\nltk_data...\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "sentiments = SentimentIntensityAnalyzer()\n",
    "data[\"Positive\"] = [sentiments.polarity_scores(i)[\"pos\"] for i in data[\"tweet\"]]\n",
    "data[\"Negative\"] = [sentiments.polarity_scores(i)[\"neg\"] for i in data[\"tweet\"]]\n",
    "data[\"Neutral\"] = [sentiments.polarity_scores(i)[\"neu\"] for i in data[\"tweet\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6bfbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## select the columns from this data that we need for the rest of the task of Twitter sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bbe7225c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               tweet  Positive  Negative  \\\n",
      "0   rt mayasolov woman shouldnt complain clean ho...     0.147     0.157   \n",
      "1   rt  boy dat coldtyga dwn bad cuffin dat hoe  ...     0.000     0.280   \n",
      "2   rt urkindofbrand dawg rt  ever fuck bitch sta...     0.000     0.577   \n",
      "3             rt cganderson vivabas look like tranni     0.333     0.000   \n",
      "4   rt shenikarobert shit hear might true might f...     0.154     0.407   \n",
      "\n",
      "   Neutral  \n",
      "0    0.696  \n",
      "1    0.720  \n",
      "2    0.423  \n",
      "3    0.667  \n",
      "4    0.440  \n"
     ]
    }
   ],
   "source": [
    "data = data[[\"tweet\", \"Positive\", \n",
    "             \"Negative\", \"Neutral\"]]\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0203cd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now letâ€™s have a look at the most frequent label assigned to the tweets according to the sentiment scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8547caa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neutral ðŸ™‚ \n"
     ]
    }
   ],
   "source": [
    "x = sum(data[\"Positive\"])\n",
    "y = sum(data[\"Negative\"])\n",
    "z = sum(data[\"Neutral\"])\n",
    "\n",
    "def sentiment_score(a, b, c):\n",
    "    if (a>b) and (a>c):\n",
    "        print(\"Positive ðŸ˜Š \")\n",
    "    elif (b>a) and (b>c):\n",
    "        print(\"Negative ðŸ˜  \")\n",
    "    else:\n",
    "        print(\"Neutral ðŸ™‚ \")\n",
    "sentiment_score(x, y, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f8f2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most of the tweets are neutral, which means they are neither positive nor negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c6a2bb34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive:  2880.086000000009\n",
      "Negative:  7201.020999999922\n",
      "Neutral:  14696.887999999733\n"
     ]
    }
   ],
   "source": [
    "print(\"Positive: \", x)\n",
    "print(\"Negative: \", y)\n",
    "print(\"Neutral: \", z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e26f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The total of neutral is way higher than negative and positive, but out of all the tweets, the negative tweets are more than the positive tweets, so we can say that most of the opinions are negative."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
